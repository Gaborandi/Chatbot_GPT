{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18e40d5b-ec74-4c08-aea6-7fba8f92e4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch1/aalamel'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/scratch1/aalamel')\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd2f819-e761-4cdb-a77f-d0fee7912c1e",
   "metadata": {},
   "source": [
    "A detailed explanation of the code:\n",
    "\n",
    "1- Import necessary libraries and modules:\n",
    "\n",
    "- torch: PyTorch library for deep learning.\n",
    "- GPT2LMHeadModel, GPT2Tokenizer, GPT2Config: Hugging Face Transformers classes for GPT-2 model, tokenizer, and configuration.\n",
    "- TextDataset, DataCollatorForLanguageModeling: Hugging Face Transformers classes for creating a dataset and data collator for language modeling.\n",
    "- Trainer, TrainingArguments: Hugging Face Transformers classes for creating a trainer and setting up training arguments.\n",
    "- pandas: Library for data manipulation and analysis.\n",
    "- Dataset: PyTorch class for creating a custom dataset.\n",
    "\n",
    "2- Read the dataset pubmed.csv using pandas and store it in df.\n",
    "\n",
    "3- Define the read_data function that takes a DataFrame and returns the \"text\" column.\n",
    "\n",
    "4- Define the TextDataset class, which inherits from PyTorch's Dataset class, to create a custom dataset for the GPT-2 model.\n",
    "\n",
    "- __init__: Initialize the dataset with the tokenized encodings.\n",
    "- __getitem__: Get an item from the dataset by its index.\n",
    "- __len__: Get the length of the dataset.\n",
    "\n",
    "5- Define the train_gpt2 function to fine-tune the GPT-2 model on the given dataset:\n",
    "\n",
    "- Load the tokenizer and configuration for the specified model name.\n",
    "- Load the GPT-2 model using the configuration.\n",
    "- Set the padding token to be the same as the end-of-sentence (EOS) token.\n",
    "- Tokenize the input texts, create the dataset, and create a data collator for language modeling.\n",
    "- Set up training arguments, including the output directory, number of epochs, batch size, and learning rate.\n",
    "- Create a trainer instance with the model, training arguments, data collator, and dataset.\n",
    "- Train the model and save the model and tokenizer to the output directory.\n",
    "\n",
    "6- Define the chat function to generate responses from the fine-tuned GPT-2 model:\n",
    "\n",
    "- Tokenize the input text and generate a response using the model.\n",
    "- Adjust the generation parameters, such as temperature and top_k, to control the randomness and diversity of the responses.\n",
    "- Decode the generated response and remove special tokens.\n",
    "\n",
    "7 - In the main block:\n",
    "\n",
    "- Read the dataset and extract the text data.\n",
    "- Set the model name to \"gpt2-medium\" and specify the output directory.\n",
    "- Fine-tune the GPT-2 model using the dataset and the specified parameters.\n",
    "- Load the fine-tuned model and tokenizer from the output directory.\n",
    "- Start an interactive chat loop, taking user input, generating responses using the chat function, and printing the responses.\n",
    "\n",
    "\n",
    "With this code, you can fine-tune a GPT-2 model on your dataset and interact with the fine-tuned model to generate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2777aa-ffba-497b-a2b5-41dd7bab7aa1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /home/aalamel/.cache/huggingface/hub/models--gpt2-medium/snapshots/425b0cc90498ac177aa51ba07be26fc2fea6af9d/vocab.json\n",
      "loading file merges.txt from cache at /home/aalamel/.cache/huggingface/hub/models--gpt2-medium/snapshots/425b0cc90498ac177aa51ba07be26fc2fea6af9d/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /home/aalamel/.cache/huggingface/hub/models--gpt2-medium/snapshots/425b0cc90498ac177aa51ba07be26fc2fea6af9d/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2-medium\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/aalamel/.cache/huggingface/hub/models--gpt2-medium/snapshots/425b0cc90498ac177aa51ba07be26fc2fea6af9d/config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/aalamel/.cache/huggingface/hub/models--gpt2-medium/snapshots/425b0cc90498ac177aa51ba07be26fc2fea6af9d/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/aalamel/.cache/huggingface/hub/models--gpt2-medium/snapshots/425b0cc90498ac177aa51ba07be26fc2fea6af9d/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/aalamel/.conda/envs/pytorch/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3\n",
      "  Number of trainable parameters = 354823168\n",
      "/local_scratch/pbs.423319.pbs02/ipykernel_2486862/4044995989.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to fine_tuned_model\n",
      "Configuration saved in fine_tuned_model/config.json\n",
      "Configuration saved in fine_tuned_model/generation_config.json\n",
      "Model weights saved in fine_tuned_model/pytorch_model.bin\n",
      "tokenizer config file saved in fine_tuned_model/tokenizer_config.json\n",
      "Special tokens file saved in fine_tuned_model/special_tokens_map.json\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file fine_tuned_model/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2-medium\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file fine_tuned_model/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at fine_tuned_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file fine_tuned_model/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is hepatitis?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: what is hepatitis?\n",
      "\n",
      "Hepatitis is a virus that causes inflammation of the liver. It is caused by a hepatitis B virus.\n",
      ", the hepatitis virus is transmitted through the blood of infected people. The virus can be spread through contact with blood, urine, feces, or saliva. Hepatotoxicity is the most common cause of liver damage in people with hepatitis. In addition, hepatitis can cause liver cancer. People with liver disease are at increased risk for developing liver cirrhosis\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  how to treat it ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: how to treat it?\n",
      "\n",
      "The best way to deal with it is to use a topical cream.\n",
      ", which is a cream that contains a lot of ingredients that are not found in the skin. It is used to remove dead skin cells and to prevent the formation of new ones. The cream is applied to the affected area and the cream will be absorbed into the bloodstream. This is the best treatment for acne. However, it can also cause irritation and redness. If you are using a\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  how to treat hepatitis ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: how to treat hepatitis?\n",
      "\n",
      "Hepatitis is a contagious disease that can be spread through contact with blood, saliva, urine, feces, or vomit. It is caused by the hepatitis B virus, which is transmitted through the bite of an infected animal.\n",
      ", and the liver is the main organ responsible for producing the virus. The liver produces the active form of the Hepatotoxic Virus (HVT) which causes liver damage and cirrhosis. Hepatic cirrosis is an\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "df = pd.read_csv('pubmed.csv')\n",
    "\n",
    "\n",
    "def read_data(df):\n",
    "    return df[\"text\"]\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "\n",
    "def train_gpt2(model_name, df, output_dir, epochs=10):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    config = GPT2Config.from_pretrained(model_name)\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name, config=config)\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    texts = df[\"text\"].tolist()\n",
    "    train_text = \"\\n\".join(texts)\n",
    "    train_encodings = tokenizer(train_text, return_tensors='pt', padding=True, truncation=True)\n",
    "    train_dataset = TextDataset(train_encodings)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=4,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "        learning_rate=5e-5,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "\n",
    "def chat(model, tokenizer, input_text, max_length=100):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, max_length=max_length, pad_token_id=tokenizer.eos_token_id, no_repeat_ngram_size=2, temperature=0.8, top_k=50)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('pubmed.csv')\n",
    "    text_data = read_data(df)\n",
    "\n",
    "    model_name = \"gpt2-medium\"\n",
    "    output_dir = \"fine_tuned_model\"\n",
    "    train_gpt2(model_name, df, output_dir, epochs=3)\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
    "    model = GPT2LMHeadModel.from_pretrained(output_dir)\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        response = chat(model, tokenizer, user_input)\n",
    "        print(\"Chatbot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b1d55e-7d9e-4700-acd8-03e981a697cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
